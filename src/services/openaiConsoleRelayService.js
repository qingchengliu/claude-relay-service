const axios = require('axios')
const logger = require('../utils/logger')
const { PassThrough } = require('stream')
const openaiConsoleAccountService = require('./openaiConsoleAccountService')

class OpenAIConsoleRelayService {
  constructor() {
    this.baseHeaders = {
      'User-Agent': 'Claude-Relay-Service/1.0',
      Accept: 'text/event-stream,application/json',
      'Accept-Encoding': 'gzip, deflate, br',
      'OpenAI-Beta': 'responses=experimental'
    }

    // éœ€è¦è¿‡æ»¤çš„è¯·æ±‚å¤´
    this.filteredRequestHeaders = new Set([
      'authorization',
      'x-api-key',
      'host',
      'content-length',
      'connection',
      'proxy-authorization',
      'content-encoding',
      'transfer-encoding',
      'te',
      'trailer',
      'upgrade'
    ])

    // éœ€è¦é€ä¼ çš„å“åº”å¤´
    this.allowedResponseHeaders = new Set([
      'openai-version',
      'x-request-id',
      'openai-processing-ms',
      'x-chatgpt-account-id',
      'x-ratelimit-limit-requests',
      'x-ratelimit-remaining-requests',
      'x-ratelimit-reset-requests'
    ])
  }

  /**
   * æ„å»ºè¯·æ±‚å¤´
   */
  buildRequestHeaders(account, originalHeaders) {
    const headers = { ...this.baseHeaders }

    // è®¾ç½®è®¤è¯å¤´
    if (account.authType === 'Bearer') {
      headers.Authorization = `Bearer ${account.apiKey}`
    } else if (account.authType === 'x-api-key') {
      headers['x-api-key'] = account.apiKey
    }

    // é€ä¼ æŸäº›åŸå§‹è¯·æ±‚å¤´
    if (originalHeaders['session_id']) {
      headers['session_id'] = originalHeaders['session_id']
    }
    if (originalHeaders['content-type']) {
      headers['Content-Type'] = originalHeaders['content-type']
    }

    // æ·»åŠ è´¦æˆ·é…ç½®çš„è‡ªå®šä¹‰å¤´
    if (account.headers && typeof account.headers === 'object') {
      Object.assign(headers, account.headers)
    }

    return headers
  }

  /**
   * æ„å»ºå“åº”å¤´
   */
  buildResponseHeaders(upstreamHeaders) {
    const headers = {}

    // åªé€ä¼ å…è®¸çš„å“åº”å¤´
    for (const [key, value] of Object.entries(upstreamHeaders)) {
      const lowerKey = key.toLowerCase()
      if (this.allowedResponseHeaders.has(lowerKey)) {
        headers[key] = value
      }
    }

    return headers
  }

  /**
   * åˆ›å»ºä»£ç†é…ç½®
   */
  createProxyConfig(account) {
    if (!account.proxy || !account.proxy.host) {
      return null
    }

    const { HttpsProxyAgent } = require('https-proxy-agent')
    const { SocksProxyAgent } = require('socks-proxy-agent')

    let proxyUrl
    if (account.proxy.type === 'socks5') {
      const auth = account.proxy.auth || ''
      proxyUrl = `socks5://${auth ? auth + '@' : ''}${account.proxy.host}:${account.proxy.port}`
      return new SocksProxyAgent(proxyUrl)
    } else {
      const auth = account.proxy.auth || ''
      proxyUrl = `http://${auth ? auth + '@' : ''}${account.proxy.host}:${account.proxy.port}`
      return new HttpsProxyAgent(proxyUrl)
    }
  }

  /**
   * è½¬å‘éæµå¼è¯·æ±‚
   */
  async relayNonStreaming(account, requestBody, originalHeaders) {
    try {
      const url = account.baseUrl + account.responsesPath
      const headers = this.buildRequestHeaders(account, originalHeaders)

      const requestConfig = {
        method: 'POST',
        url,
        headers,
        data: requestBody,
        timeout: 60000,
        validateStatus: null
      }

      // æ·»åŠ ä»£ç†é…ç½®
      const proxyAgent = this.createProxyConfig(account)
      if (proxyAgent) {
        requestConfig.httpsAgent = proxyAgent
      }

      logger.info(`ğŸ”„ Forwarding non-streaming request to OpenAI Console: ${url}`)
      const response = await axios(requestConfig)

      // æ„å»ºå“åº”
      const responseHeaders = this.buildResponseHeaders(response.headers)
      
      // è®°å½•ä½¿ç”¨æƒ…å†µ
      if (response.data && response.data.usage) {
        await this.recordUsage(account.id, response.data.usage, requestBody.model)
      }

      return {
        status: response.status,
        headers: responseHeaders,
        data: response.data
      }
    } catch (error) {
      logger.error('âŒ OpenAI Console relay error (non-streaming):', error)
      throw this.handleError(error)
    }
  }

  /**
   * è½¬å‘æµå¼è¯·æ±‚
   */
  async relayStreaming(account, requestBody, originalHeaders) {
    const url = account.baseUrl + account.responsesPath
    const headers = this.buildRequestHeaders(account, originalHeaders)
    const stream = new PassThrough()

    try {
      const requestConfig = {
        method: 'POST',
        url,
        headers,
        data: requestBody,
        responseType: 'stream',
        timeout: 60000,
        validateStatus: null
      }

      // æ·»åŠ ä»£ç†é…ç½®
      const proxyAgent = this.createProxyConfig(account)
      if (proxyAgent) {
        requestConfig.httpsAgent = proxyAgent
      }

      logger.info(`ğŸ”„ Forwarding streaming request to OpenAI Console: ${url}`)
      const response = await axios(requestConfig)

      // æ„å»ºå“åº”å¤´
      const responseHeaders = this.buildResponseHeaders(response.headers)
      responseHeaders['content-type'] = 'text/event-stream'
      responseHeaders['cache-control'] = 'no-cache'
      responseHeaders['connection'] = 'keep-alive'

      // å¤„ç†æµå¼æ•°æ®
      let usage = null
      let buffer = ''

      response.data.on('data', (chunk) => {
        try {
          buffer += chunk.toString()
          const lines = buffer.split('\n')
          buffer = lines.pop() || ''

          for (const line of lines) {
            if (line.trim() === '') {
              stream.write('\n')
              continue
            }

            // è½¬å‘åŸå§‹ SSE è¡Œ
            stream.write(line + '\n')

            // è§£æ usage æ•°æ®
            if (line.startsWith('event: response.done') || line.startsWith('event: response.completed')) {
              // æŸ¥æ‰¾ä¸‹ä¸€ä¸ª data è¡Œä¸­çš„ usage
              const dataLineIndex = lines.indexOf(line) + 1
              if (dataLineIndex < lines.length) {
                const dataLine = lines[dataLineIndex]
                if (dataLine.startsWith('data: ')) {
                  try {
                    const data = JSON.parse(dataLine.substring(6))
                    if (data.usage) {
                      usage = data.usage
                    }
                  } catch (e) {
                    // å¿½ç•¥è§£æé”™è¯¯
                  }
                }
              }
            } else if (line.startsWith('data: ')) {
              try {
                const data = JSON.parse(line.substring(6))
                if (data.usage) {
                  usage = data.usage
                }
              } catch (e) {
                // å¿½ç•¥è§£æé”™è¯¯
              }
            }
          }
        } catch (error) {
          logger.error('âŒ Error processing stream chunk:', error)
        }
      })

      response.data.on('end', async () => {
        try {
          // å¤„ç†ç¼“å†²åŒºä¸­å‰©ä½™çš„æ•°æ®
          if (buffer.trim()) {
            stream.write(buffer)
            
            // æ£€æŸ¥æœ€åçš„æ•°æ®æ˜¯å¦åŒ…å« usage
            if (buffer.startsWith('data: ')) {
              try {
                const data = JSON.parse(buffer.substring(6))
                if (data.usage) {
                  usage = data.usage
                }
              } catch (e) {
                // å¿½ç•¥è§£æé”™è¯¯
              }
            }
          }

          stream.end()

          // è®°å½•ä½¿ç”¨æƒ…å†µ
          if (usage) {
            await this.recordUsage(account.id, usage, requestBody.model)
          }

          logger.info('âœ… OpenAI Console streaming completed')
        } catch (error) {
          logger.error('âŒ Error in stream end handler:', error)
          stream.end()
        }
      })

      response.data.on('error', (error) => {
        logger.error('âŒ OpenAI Console stream error:', error)
        const errorEvent = this.formatSSEError(error)
        stream.write(errorEvent)
        stream.end()
      })

      return {
        status: response.status,
        headers: responseHeaders,
        stream
      }
    } catch (error) {
      logger.error('âŒ OpenAI Console relay error (streaming):', error)
      const errorEvent = this.formatSSEError(error)
      stream.write(errorEvent)
      stream.end()
      
      return {
        status: error.response?.status || 500,
        headers: { 'content-type': 'text/event-stream' },
        stream
      }
    }
  }

  /**
   * ä¸»è½¬å‘æ–¹æ³•
   */
  async relay(account, requestBody, originalHeaders) {
    // æ›´æ–°è´¦æˆ·æœ€åä½¿ç”¨æ—¶é—´
    await openaiConsoleAccountService.updateLastUsedTime(account.id)

    if (requestBody.stream) {
      return await this.relayStreaming(account, requestBody, originalHeaders)
    } else {
      return await this.relayNonStreaming(account, requestBody, originalHeaders)
    }
  }

  /**
   * è®°å½•ä½¿ç”¨æƒ…å†µ
   */
  async recordUsage(accountId, usage, model) {
    try {
      const redisClient = require('../models/redis')
      
      // è®°å½•è´¦æˆ·çº§åˆ«çš„ä½¿ç”¨ç»Ÿè®¡
      await redisClient.incrementAccountUsage(
        accountId,
        usage.total_tokens || 0,
        usage.prompt_tokens || 0,
        usage.completion_tokens || 0,
        usage.cache_creation_input_tokens || 0,
        usage.cache_read_input_tokens || 0,
        model || 'unknown',
        false
      )

      logger.info(`ğŸ“Š Recorded usage for OpenAI Console account ${accountId}:`, usage)
    } catch (error) {
      logger.error('âŒ Failed to record usage:', error)
    }
  }

  /**
   * æ ¼å¼åŒ– SSE é”™è¯¯äº‹ä»¶
   */
  formatSSEError(error) {
    const errorData = {
      error: {
        type: error.response?.status === 429 ? 'rate_limit_error' : 'api_error',
        message: error.response?.data?.error?.message || error.message || 'Internal server error',
        code: error.response?.status || 500
      }
    }

    return `event: error\ndata: ${JSON.stringify(errorData)}\n\n`
  }

  /**
   * å¤„ç†é”™è¯¯
   */
  handleError(error) {
    if (error.response) {
      // é€ä¼ ä¸Šæ¸¸é”™è¯¯
      const customError = new Error(error.response.data?.error?.message || error.message)
      customError.status = error.response.status
      customError.data = error.response.data
      return customError
    } else if (error.code === 'ECONNREFUSED') {
      const customError = new Error('OpenAI Console service unavailable')
      customError.status = 503
      return customError
    } else if (error.code === 'ETIMEDOUT') {
      const customError = new Error('Request timeout')
      customError.status = 504
      return customError
    } else {
      const customError = new Error('Internal relay error')
      customError.status = 500
      return customError
    }
  }
}

module.exports = new OpenAIConsoleRelayService()